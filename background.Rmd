---
title: "Kerr Mess Around"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(sf)
library(readxl)
library(lubridate)
```

## Background

This script explores Kerr Reservoir profile data and some upstream ambient trend site data. Scott gets repeated requests for data from DGIF for striper population inquiries and this tool may be expanded to statewide applicability in the future.

## Data Pull (outside R)

The below dataset was pulled outside of R (using Logi Lakes Field Data pull with BRRO 2010-Sept 2019 specified) because DEQ does not allow CEDS connection.

Because Logi query only goes to .xls and has a lot of front matter, I saved original data pull in data/original and deleted front matter and saved each sheet in data/

```{r data in BRRO}
BRRO1 <- read_csv('data/BRRO2010_Sept2019_1.csv')
BRRO2 <- read_csv('data/BRRO2010_Sept2019_2.csv')

# Combine to one dataset
BRRO <- bind_rows(BRRO1,BRRO2) %>%
  filter_all(any_vars(!is.na(.)))  # clean up empty rows

# clean up workspace
rm(BRRO1);rm(BRRO2)

# What stations available?
unique(BRRO$ID)
```


So no Kerr available with BRRO data pull. Rinse and repeat with PRO.

```{r data in PRO}
PRO <- read_csv('data/PRO2010_Sept2019.csv') %>%
  filter_all(any_vars(!is.na(.)))  # clean up empty rows

# What stations available?
unique(PRO$ID)
```

Limit to just Kerr Stations. kerrSites are from Paula's 2020 Stations table.

```{r filter PRO}
kerrSites <- read_csv('C:/HardDriveBackup/R/GitHub/LakesAssessment2020/app2020/LakeAssessmentApp_citmon/processedStationData/lakeStations2020_BRRO_citmonNonAgency.csv') %>%
  filter(GNIS_Name == 'John H. Kerr Reservoir') %>%
  filter(STATION_TYPE_1 == 'L' | STATION_TYPE_2 == 'L' | STATION_TYPE_3 == 'L') # keep only lake stations ( duplicated 4ABMA002.00 relic of assessment process)


kerr <- filter(PRO, ID %in% kerrSites$FDT_STA_ID) %>%
  bind_rows(filter(BRRO, ID %in% kerrSites)) %>%
  rename('FDT_STA_ID' = 'ID') %>%
  left_join(dplyr::select(kerrSites, FDT_STA_ID, Latitude, Longitude, SEC, CLASS, 
                          SPSTDS, PWS, Trout, SEC187, Chlorophyll_A_limit,
                          TPhosphorus_limit, Assess_TYPE), by = 'FDT_STA_ID') %>% # join lat/lon information and WQS info
  mutate(`Date Time` = as.POSIXct(`Date Time`, format = "%m/%d/%Y %l:%M %p"))

```


The data pull isn't perfect because these sites still need to be pulled:
```{r missing}
# Sites that still need to be pulled
kerrSites$FDT_STA_ID[!(kerrSites$FDT_STA_ID %in% unique(kerr$FDT_STA_ID))]
```


I pulled those individually from logi using the 'Field Data Station - McLeod' query limiting by Jan1 2010- Sept 9 2019. Below are efforts to first combine the sites...

Also manually added in Location (CEDS station description) bc queries generated different fields

```{r organize individual sites}
X4ABHB004.40 <- read_csv('data/4ABHB004.40_Sept2019.csv')
X4ABMA002.00 <- read_csv('data/4ABMA002.00_Sept2019.csv')
X4ABST001.13 <- read_csv('data/4ABST001.13_Sept2019.csv')
X4ADAN000.00 <- read_csv('data/4ADAN000.00_Sept2019.csv')
X4AROA043.14 <- read_csv('data/4AROA043.14_Sept2019.csv')

sites <- rbind(X4ABHB004.40, X4ABMA002.00, X4ABST001.13, X4ADAN000.00, X4AROA043.14) %>%
  rename('FDT_STA_ID' = 'ID') %>%
  dplyr::select(-c(Latitude, Longitude)) %>% # accept assessor-approved lat/lon
  left_join(dplyr::select(kerrSites, FDT_STA_ID, STA_DESC, Latitude, Longitude, SEC, CLASS, 
                          SPSTDS, PWS, Trout, SEC187, Chlorophyll_A_limit,
                          TPhosphorus_limit, Assess_TYPE), by = 'FDT_STA_ID') %>%# join lat/lon information and WQS info
  rename('Location' = 'STA_DESC') %>%
  mutate(`Date Time` = as.POSIXct(`Date Time`, format='%m/%d/%Y'))

names(kerr)[!(names(kerr) %in% names(sites))] # make sure all fields we need to populate kerr are in sites

sites <- dplyr::select(sites, names(kerr))
```


... and then combine the site datasets with the rest of the kerr lake profile data.


```{r combine all kerr}

kerr <- bind_rows(kerr,sites)

# double check all of Paula's Kerr sites are in there
all(unique(kerrSites$FDT_STA_ID) %in% unique(kerr$FDT_STA_ID))
```

Save Data.

```{r}
write.csv(kerr, 'data/Kerr2010-Sept2019.csv', row.names = F)
```












Get one profile to play with 4AROA022.52

```{r one profile}
kerr1 <- filter(kerr, FDT_STA_ID == '4AROA022.52')

kerr1DO <- select(kerr1, ID, `Date Time`, Depth, `Do Probe`)

```

Build a filled contour plot for the profile.

```{r filled contour}

data <- as.data.frame(kerr1DO)
parameter <- 'Do Probe'
depth <- 'Depth'
date <- 'Date Time'

data = data[!is.na(data[, parameter]) & !is.na(data[, depth]), ]
data[, date] = as.Date(data[, date], format = "%m/%d/%Y %l:%M %p")
int = akima::interp(data[, date], data[, depth], data[, parameter], duplicate = "strip", linear = TRUE)
min_date = min(data[, date], na.rm = T)
max_date = max(data[, date], na.rm = T)
depth_units = "m"

param_lab = "DO"
param_units = 'mg/L'
title = paste0(param_lab, " (", param_units, ")")

criteria=4 # applicable WQS
if (missing(criteria)) {
    criteria = c(0, 10, 15, 20, 25, 30)
  }

filled.contour(int, color.palette = function(n) topo.colors(n, rev=T), 
               xlim = c(min_date - 1, max_date + 1), ylim = rev(range(data[, depth])), 
               ylab = paste0("Depth (", depth_units, ")"), 
               main = title, 
               plot.axes = {
                 contour(int, levels = criteria, lwd = 2, col = "red", 
                         drawlabels = TRUE, axes = FALSE, frame.plot = FALSE, 
                         add = TRUE, labcex = 2)
                 axis(1, at = unique(data[, date]), labels = unique(data[,date]), par(las = 2))
                 axis(2)}
               )
```

Cool, now let's try to beef this function up with some date options and shiny slider capabilities.

```{r functionalize}

data <- as.data.frame(kerr1DO)
parameter <- 'Do Probe'
depth <- 'Depth'
date <- 'Date Time'

data = data[!is.na(data[, parameter]) & !is.na(data[, depth]), ]
data[, date] = as.Date(data[, date], format = "%m/%d/%Y %l:%M %p")
int = akima::interp(data[, date], data[, depth], data[, parameter], duplicate = "strip", linear = TRUE)
min_date = min(data[, date], na.rm = T)
max_date = max(data[, date], na.rm = T)
depth_units = "m"

param_lab = "DO"
param_units = 'mg/L'
title = paste0(param_lab, " (", param_units, ")")

criteria=4 # applicable WQS
if (missing(criteria)) {
    criteria = c(0, 10, 15, 20, 25, 30)
  }

data = kerr1
dateCol <- 'Date Time'
dateTimeFormat <- "%m/%d/%Y %l:%M %p"
depthCol <- 'Depth'
parameterCol = 'Do Probe'
depth_units = "m"
reverseColorPalette = TRUE


interpolateData <- function(data = data,
                            dateCol = NA,
                            dateTimeFormat = '%M-%D-%Y',
                            depthCol= NA,
                            parameterCol = NA,
                            depth_units = NA){
  data <- as.data.frame(data) # only work with dataframes, tibbles etc. are difficult for interpolation method
  
  # manipulate dataset to standardized format
  data1 <- mutate(data, Date = as.Date(get(dateCol), format = dateTimeFormat),
           Depth = get(depthCol),
           parameter = get(parameterCol) ) %>%
    dplyr::select(Date, Depth, parameter) %>% # just the columns we want
    drop_na() # get rid of any missing data bc everything at this point is critical for interpolation
  
  # interpolate between sample events
  int = akima::interp(data1[, 'Date'], data1[, 'Depth'], data1[, 'parameter'], duplicate = "strip", linear = TRUE)
  
  # establish plotting date min/max 
  #min_date <- min(data1[, 'Date'], na.rm = T)
  #max_date <- max(data1[, 'Date'], na.rm = T)
  
  
  results <- list(data1,int)
  return(results)

}  
  

int = int_WQX; reverseColorPalette = TRUE; criteria = 4

contour.heatmap <- function(int = int,
                            reverseColorPalette = FALSE,
                            title = NA,
                            criteria = NA,
                            depth_units = "m"){
  filled.contour(int[[2]], color.palette = function(n) topo.colors(n, rev= reverseColorPalette), 
               xlim = c(min(int[[1]][, 'Date']) - 1, max(int[[1]][, 'Date']) + 1), 
               ylim = rev(range(int[[1]][, 'Depth'])), 
               ylab = paste0("Depth (", depth_units, ")"), 
               main = title, 
               plot.axes = {
                 contour(int[[2]], levels = criteria, lwd = 2, col = "red", 
                         drawlabels = TRUE, axes = FALSE, frame.plot = FALSE, 
                         add = TRUE, labcex = 2)
                 axis(1, at = unique(int[[1]][, 'Date']), labels = unique(int[[1]][,'Date']), par(las = 2))
                 axis(2)}
               )
  
}

int <- interpolateData(data = kerr1, # to adjust date range you need to limit input data
                       dateCol = 'Date Time',
                       dateTimeFormat = "%m/%d/%Y %l:%M %p",
                       depthCol = 'Depth',
                       parameterCol = 'Do Probe',
                       depth_units = "m")


int_short <- interpolateData(data = kerr1[1:118,], # to adjust date range you need to limit input data
                       dateCol = 'Date Time',
                       dateTimeFormat = "%m/%d/%Y %l:%M %p",
                       depthCol = 'Depth',
                       parameterCol = 'Do Probe',
                       depth_units = "m")

contour.heatmap(int, reverseColorPalette = TRUE, criteria = 4)
contour.heatmap(int_short, reverseColorPalette = TRUE, criteria = 4)


```



So now let's try to scrape data straight from WQX to put into our plotting function.

```{r webscrape}

library(wqTools) # use Utah's webscraping function

kerr1_nr <- readWQP(type="narrowresult", siteid="21VASWCB-4AROA038.49", print=F)
kerr1_act <- readWQP(type="activity", siteid="21VASWCB-4AROA038.49", print=F)
kerr1_nr_act <- merge(kerr1_nr, kerr1_act, all.x=T) %>%
  filter(!is.na(ResultMeasureValue)) #Subset to profile data EVJ change to ResultMeasureValue to get populated data lines


# function to simplify formatting
WQX_wide <- function(data = data){
   #make sure only one tz
  if(length(unique(data$ActivityStartTime.TimeZoneCode)) == 1){  tz <- unique(data$ActivityStartTime.TimeZoneCode)
  }else{
    stop("More than one time zone present, clean data.")
  }

  # Get columns of interest
  data1 <- dplyr::select(data, ActivityStartDate, ActivityStartTime.Time, ActivityStartTime.TimeZoneCode, MonitoringLocationIdentifier,
                         CharacteristicName, ResultSampleFractionText, ResultMeasureValue, ResultMeasure.MeasureUnitCode, ActivityDepthHeightMeasure.MeasureValue,
                         ActivityDepthHeightMeasure.MeasureUnitCode,ActivityLocation.LatitudeMeasure, ActivityLocation.LongitudeMeasure) %>%
    mutate(CharacteristicName_unit = paste(CharacteristicName, ' [', ResultSampleFractionText,']',
                                           ' (',ResultMeasure.MeasureUnitCode,')',sep = ''), #make one name for each parameter to not lose units
           DateTime = as.POSIXct(paste(ActivityStartDate, ActivityStartTime.Time, sep = ' '), format = '%Y-%m-%d %H:%M:%S', tz = tz),
           Depth = ActivityDepthHeightMeasure.MeasureValue) %>%
    unite(SampleDate_Depth, DateTime, Depth, sep = '.._/_!_..')  # use unique string to avoid separation issues

  
  # Collapse a smaller version of this to not lose wide data 
  data2 <- dplyr::select(data1, CharacteristicName_unit, SampleDate_Depth, ResultMeasureValue) %>%
    distinct(CharacteristicName_unit, SampleDate_Depth, .keep_all = TRUE) %>%
    # spread by activity (parameter collected)
    spread(CharacteristicName_unit,ResultMeasureValue,  convert = TRUE, drop=FALSE) # convert = TRUE keeps original data format
  
  data3 <- dplyr::select(data1, SampleDate_Depth, MonitoringLocationIdentifier, ActivityLocation.LatitudeMeasure, ActivityLocation.LongitudeMeasure) %>%
    distinct() %>%
    left_join(data2, by='SampleDate_Depth') %>%
    separate(SampleDate_Depth, c('DateTime', "Depth"), sep= '.._/_!_..') %>%
    # update column types
    mutate(DateTime = as.POSIXct(`DateTime`, format = '%Y-%m-%d %H:%M:%S', tz = tz),
           Depth = as.numeric(Depth)) %>%
    rename('LatitudeDD' = 'ActivityLocation.LatitudeMeasure',
           'LongitudeDD' = 'ActivityLocation.LongitudeMeasure')%>%
  arrange(DateTime)
  
  return(data3)
  
  }

zz <- WQX_wide(kerr1_nr_act) %>%
  mutate(Year = year(DateTime))


rm(kerr1_nr_act);rm(kerr1_nr);rm(kerr1_act)  
```  
  
profiles avaialble for 1989, 1994, 2000, 2005, 2010, 2012, 2014, 2016, 2018, 2019
  

```{r}
# interpolate contours for DO
int_WQX <- interpolateData(data = filter(zz, Year==1989 & !is.na('Temperature, water [Total] (deg C)')), # to adjust date range you need to limit input data
                           dateCol = 'DateTime',
                           dateTimeFormat = '%Y-%m-%d %H:%M:%S',
                           depthCol = "Depth",
                           parameterCol = 'Temperature, water [Total] (deg C)',
                           depth_units = "m")

contour.heatmap(int_WQX, reverseColorPalette = FALSE, criteria = 20)


```

Try with plotly

```{r}
library(plotly)

#int3 <- readRDS('int_2010-2019_T.RDS')
contour.heatmap(int3, reverseColorPalette = F, criteria = 30)


plot_ly(x=int3[[2]]$x, y=int3[[2]]$y, z = int3[[2]]$z, type = "heatmap") %>%
  layout(#xaxis = list(autorange = "reversed"),
    yaxis = list(autorange = "reversed"))

```

Back to old fashioned plotly scatter plot with color gradient?

```{r}
plot_ly(data = zz, x = ~DateTime, y = ~Depth, 
        color = ~`Temperature, water [Total] (deg C)`, size = 5) %>%
  layout(yaxis = list(autorange = "reversed"))

```


```{r}
plot_ly(data = zz3, x = ~DateTime, y = ~Depth, 
        color = ~`Temperature, water [Total] (deg C)`, size = 5) %>%
  layout(yaxis = list(autorange = "reversed"))

```



next step: kriging, could be applied to various depths across lake or by profile?

https://rpubs.com/nabilabd/118172


https://wlperry.github.io/bioblog/2017/12/25/lake-heat-maps-with-ggplot/

```{r}
#load packages
library(tidyverse)
library(lubridate)
library(ggplot2)
library(scales)
library(colorRamps)
library(akima)
library(plotly)


# clean up data
zz_clean.df <- zz %>% # use this df for rest of commands
  dplyr::select(Year, DateTime, Depth,`Temperature, water [Total] (deg C)`) %>%
  filter(`Temperature, water [Total] (deg C)`>=0) %>% # select data that is greater or equal to 0
  filter(!is.na(`Temperature, water [Total] (deg C)`)) %>%# remove all NA values
  mutate(Date = as.Date(DateTime))

```

Too much time/space to interpolate all these windows at once, so restrict to 1989-1994, 2000-2005, 2010-2014, 2016-2019

```{r}
zz1989 <- filter(zz_clean.df, Year %in% 2018:2019) %>%
  select(Date, Depth, `Temperature, water [Total] (deg C)`)

#zz1989_1 <- zz1989
int1989 <- interp(x = zz1989$Date,
         y = zz1989$Depth,
         z = zz1989$`Temperature, water [Total] (deg C)`,
         xo = seq(min(zz1989$Date), max(zz1989$Date), by = 1),
         yo = seq(min(zz1989$Depth), max(zz1989$Depth), by = 1),
         extrap=FALSE,
         linear=TRUE)


# this converts the interpolated data into a dataframe
interp.df <- interp2xyz(int1989, data.frame = TRUE)


# clean up dates using dplyr
interp.df <- interp.df %>%
  filter(x %in% as.numeric(zz1989_1$Date)) %>% # this matches the interpolated data with what is in the main dataframe to remove dates we dont have
  mutate(date = as_date(x)) %>% # interp turned dates into integers and this converts back
  mutate(day = day(date)) %>% # create day varaible for plotting
  mutate(year = year(date)) %>% # create a four digit year column
  select(-x) %>% #remove x column
  rename(depth=y, wtemp=z) #rename variables


# plot our interpolated data
ggplotly(ggplot(interp.df, aes(x = date, y = depth, z = wtemp, fill = wtemp)) +
  geom_raster() +
  scale_y_reverse(expand=c(0,0)) +
  scale_fill_gradientn(colours=matlab.like(10), na.value = 'gray', name="Water\nTemp \nÂºC") + 
  scale_x_date(date_breaks = "1 week", 
               # limits = as_date(c('2016-12-06','2017-02-25')),
               labels=date_format("%b-%d"), expand=c(0,0)) + 
  ylab("Depth (m)") +
  xlab("") )
```


```{r}
# interolated watertemp with depth, linear
# jsut a note here that the x interpretation step of 1 works with day data as it is using the 
# number of days. The issue comes up when you want to use time. The thing to remember here is 
# time in R is the number of seconds since 1970-01-01 00:00:00 so if you do hours you would use
# 3600 seconds rather than 1
zz_interp.df <- interp(x = zz_clean.df$DateTime,
                       y = zz_clean.df$Depth,
                       z = zz_clean.df$`Temperature, water [Total] (deg C)`,
                       xo = seq(min(zz_clean.df$DateTime), max(zz_clean.df$DateTime), by = 1),
                       yo = seq(min(zz_clean.df$Depth), max(zz_clean.df$Depth), by = 0.2),
                       extrap=FALSE,
                       linear=TRUE)

```

